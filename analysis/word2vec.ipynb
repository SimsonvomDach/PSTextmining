{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Analysis and Preprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For File Navigation\n",
    "import os\n",
    "\n",
    "# Our NLP Library which includes Word2Vec algorithm\n",
    "import gensim\n",
    "\n",
    "# For tokenizing corpus into sentences\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# For tokenizing the sentences into words, lowercase them and remove punctuation marks\n",
    "from gensim.utils import simple_preprocess\n",
    "# For removing stopwords\n",
    "from gensim.parsing.preprocessing import remove_stopwords\n",
    "\n",
    "# Generate WordCloud\n",
    "from wordcloud import WordCloud\n",
    "\n",
    "# PCA\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Advanced Visualization for word vectors\n",
    "import plotly.express as px\n",
    "from plotly.offline import init_notebook_mode\n",
    "init_notebook_mode(connected=True)\n",
    "\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Harry_Potter_Movies/Dialogue.csv', encoding='ISO-8859-1')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuations(text):\n",
    "    for punctuation in string.punctuation:\n",
    "        text = text.replace(punctuation,\" \")\n",
    "    return text\n",
    "\n",
    "df[\"Tokens\"] = df[\"Dialogue\"].str.lower().apply(remove_punctuations).apply(nltk.word_tokenize)\n",
    "df[\"Tokens\"]\n",
    "df\n",
    "\n",
    "dialoge = df[\"Tokens\"].tolist()\n",
    "filtered_tokens = []\n",
    "for line in dialoge:\n",
    "    line_list = []\n",
    "    for token in line:\n",
    "        if token.lower() not in stopwords.words('english') and token not in string.punctuation:\n",
    "            line_list.append(token)\n",
    "\n",
    "    filtered_tokens.append(line_list)\n",
    "\n",
    "df[\"Tokens\"] = filtered_tokens\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story = []\n",
    "\n",
    "corpus = df['Dialogue'].tolist()\n",
    "raw = ''.join(corpus)\n",
    "raw_sent = nltk.sent_tokenize(raw)\n",
    "for sent in raw_sent:\n",
    "        # 3 - Removal of stopwords\n",
    "        sent = remove_stopwords(sent)\n",
    "        \n",
    "        # 4 - Removal of punctuation marks \n",
    "        # 5 - Tokenization of sentences to words\n",
    "        story.append(simple_preprocess(sent))\n",
    "story = [x for x in story if x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_concat = []\n",
    "token_join = []\n",
    "\n",
    "for token in story:\n",
    "    tokenjoin = ' '.join(token)\n",
    "    token_join.append(tokenjoin)\n",
    "storyjoin = ' '.join(token_join)\n",
    "story_concat.append(storyjoin)\n",
    "\n",
    "story_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Word2Vec(\n",
    "    vector_size=1000,\n",
    "    window=5,  \n",
    "    min_count=3,\n",
    "    workers=4,\n",
    "    sg=1,\n",
    ")\n",
    "model.build_vocab(filtered_tokens, progress_per=1000)\n",
    "model.train(filtered_tokens, total_examples=model.corpus_count, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.most_similar('snape')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(['dumbledore', 'ron', 'arthur', 'fred', 'george', 'ginny'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(['harry', 'ron', 'hermione', 'malfoy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.doesnt_match(['gryffindor', 'slytherin', 'hufflepuff', 'ravenclaw', 'voldemort'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('ginny', 'ron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('hermione', 'ron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.similarity('voldemort', 'ron')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.wv.get_normed_vectors().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Amount of unique words\n",
    "len(model.wv.index_to_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import os\n",
    "import re\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from stop_words import get_stop_words\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from gensim.models.doc2vec import TaggedDocument"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_characters = pd.read_csv(\"Harry_Potter_Movies/Characters.csv\", encoding='ISO-8859-1')\n",
    "df_chardialogues = pd.merge(df, df_characters, on='Character ID', how=\"left\")\n",
    "df_dialHarry = df_chardialogues[df_chardialogues['Character ID'] == 1]\n",
    "dialHarry = df_dialHarry['Tokens'].tolist()\n",
    "df_dialRon = df_chardialogues[df_chardialogues['Character ID'] == 2]\n",
    "df_dialHerm = df_chardialogues[df_chardialogues['Character ID'] == 3]\n",
    "df_dialDumbl = df_chardialogues[df_chardialogues['Character ID'] == 4]\n",
    "df_dialHagr = df_chardialogues[df_chardialogues['Character ID'] == 5]\n",
    "df_dialSnape = df_chardialogues[df_chardialogues['Character ID'] == 6]\n",
    "df_dialVold = df_chardialogues[df_chardialogues['Character ID'] == 9]\n",
    "\n",
    "df_characters_list = [df_dialHarry,df_dialRon,df_dialHerm,df_dialDumbl,df_dialHagr,df_dialSnape,df_dialVold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dialHarry"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = []\n",
    "token = []\n",
    "list = []\n",
    "for line in dialHarry:\n",
    "    if line.__class__ == list:\n",
    "        list = ''.join(line)\n",
    "        token = ' '.join(list)\n",
    "        doc.append(token)\n",
    "    else:\n",
    "        token = ' '.join(line)\n",
    "        doc.append(token)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "document = []\n",
    "yes = []\n",
    "no = []\n",
    "for line in doc:\n",
    "    yes = ''.join(line)\n",
    "    print(yes)\n",
    "    no.append(yes)\n",
    "    print(no)\n",
    "document.append(no)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_concat = []\n",
    "token_join = []\n",
    "\n",
    "for token in doc:\n",
    "    tokenjoin = ''.join(token)\n",
    "    token_join.append(tokenjoin)\n",
    "storyjoin = ' '.join(token_join)\n",
    "story_concat.append(storyjoin)\n",
    "\n",
    "story_concat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "story_concat[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "documents = [TaggedDocument(words=story_concat[0], tags=[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = RegexpTokenizer(r'\\w+')\n",
    "en_stop = get_stop_words('en')\n",
    "p_stemmer = PorterStemmer()\n",
    "\n",
    "taggeddoc = []\n",
    " \n",
    "texts = []\n",
    "for index,i in enumerate(df_characters_list):\n",
    "    # for tagged doc\n",
    "    wordslist = []\n",
    "    tagslist = []\n",
    "\n",
    "    # clean and tokenize document string\n",
    "    raw = i.lower()\n",
    "    tokens = tokenizer.tokenize(raw)\n",
    "\n",
    "    # remove stop words from tokens\n",
    "    stopped_tokens = [i for i in tokens if not i in en_stop]\n",
    "\n",
    "    # remove numbers\n",
    "    number_tokens = [re.sub(r'[\\d]', ' ', i) for i in stopped_tokens]\n",
    "    number_tokens = ' '.join(number_tokens).split()\n",
    "\n",
    "    # stem tokens\n",
    "    stemmed_tokens = [p_stemmer.stem(i) for i in number_tokens]\n",
    "    # remove empty\n",
    "    length_tokens = [i for i in stemmed_tokens if len(i) > 1]\n",
    "    # add tokens to list\n",
    "    texts.append(length_tokens)\n",
    "\n",
    "    td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(stemmed_tokens))).split(),str(index))\n",
    "    # for later versions, you may want to use: td = TaggedDocument(gensim.utils.to_unicode(str.encode(' '.join(stemmed_tokens))).split(),[str(index)])\n",
    "    taggeddoc.append(td)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.Doc2Vec(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.train(documents, total_examples=model.corpus_count, epochs=25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model['aunt']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c6e4e9f98eb68ad3b7c296f83d20e6de614cb42e90992a65aa266555a3137d0d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
